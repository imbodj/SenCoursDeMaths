\input{../common}
\input{../bibliography}

\begin{document}
  %<*content>
  \lesson{algebra}{159}{Formes linéaires et dualité en dimension finie. Exemples et applications.}

  Soit $E$ un espace vectoriel sur un corps commutatif $\mathbb{K}$ de dimension finie $n$.

  \subsection{Dual d'un espace vectoriel}

  \subsubsection{Formes linéaires, espace dual}

  \reference[ROM21]{441}

  \begin{definition}
    Une \textbf{forme linéaire} sur $E$ est une application linéaire de $E$ dans $\mathbb{K}$. L'espace $\mathcal{L}(E, \mathbb{K})$ formé par l'ensemble des formes linéaires sur $E$ est appelé \textbf{dual} de $E$ et est noté $E^*$.
  \end{definition}

  \begin{example}
    \label{159-1}
    \begin{itemize}
      \item Soit $\mathcal{B} = (e_1, \dots, e_n)$ une base de $E$. Alors pour tout $j \in \llbracket 1, n \rrbracket$, la projection
      \[ p_j : \sum_{i=1}^{n} x_i e_i \mapsto x_j \]
      est une forme linéaire.
      \item Toute combinaison linéaire de formes linéaires est une forme linéaire.
    \end{itemize}
  \end{example}

  \begin{remark}
    Une forme linéaire non nulle sur $E$ est surjective.
  \end{remark}

  \begin{definition}
    On appelle \textbf{hyperplan} de $E$, le noyau d'une forme linéaire non nulle sur $E$.
  \end{definition}

  \begin{proposition}
    \begin{enumerate}[label=(\roman*)]
      \item Un hyperplan de $E$ est un sous-espace de $E$ supplémentaire d'une droite.
      \item Deux formes linéaires non nulles définissent le même hyperplan si et seulement si elles sont liées.
    \end{enumerate}
  \end{proposition}

  \subsubsection{Bases duales}

  \reference[GOU21]{133}

  \begin{definition}
    En reprenant les notations de la \cref{159-1}, les projections $p_i$ sont les \textbf{formes linéaires coordonnées}. On note $\forall i \in \llbracket 1, n \rrbracket$, $p_i = e_i^*$. La famille $\mathcal{B}^* = (e_1, \dots, e_n)$ est appelée \textbf{base duale} de $\mathcal{B}$.
  \end{definition}

  \begin{remark}
    Soit $\mathcal{B} = (e_1, \dots, e_n)$ une base de $E$. Pour tout $i, j \in \llbracket 1, n \rrbracket$, on a
    \[ e_i^*(e_j) = \delta_{i,j} = \begin{cases} 1 &\text{ si } i = j \\ 0 &\text{ sinon} \end{cases} \]
  \end{remark}

  \begin{theorem}
    Soit $\mathcal{B} = (e_1, \dots, e_n)$ une base de $E$. Alors, la base duale $\mathcal{B}^*$ est une base de $E^*$.
  \end{theorem}

  \begin{corollary}
    \begin{enumerate}[label=(\roman*)]
      \item $E^*$ est un espace vectoriel de dimension $n$.
      \item Pour tout $\varphi \in E^*$, on a $\varphi = \sum_{n=1}^n \varphi(e_i) e_i^*$.
    \end{enumerate}
  \end{corollary}

  \reference[ROM21]{446}

  \begin{corollary}
    Tout hyperplan de $E$ est de dimension $n-1$.
  \end{corollary}

  \reference[GOU20]{325}

  \begin{example}
    Soit $U \subseteq \mathbb{R}^n$ un ouvert. Soit $f : U \rightarrow \mathbb{R}$ différentiable en $a \in U$. Alors,
    \[ \mathrm{d}f_a = \sum_{i=1}^n \frac{\partial f}{\partial x_i}(a) e_i^* \]
    où $(e_i^*)_{i \in \llbracket 1, n \rrbracket}$ est la base duale de la base canonique $(e_i)_{i \in \llbracket 1, n \rrbracket}$ de $\mathbb{R}^n$.
  \end{example}

  \subsubsection{Bidual}

  \reference[GOU21]{133}

  \begin{definition}
    On appelle \textbf{bidual} de $E$ le dual $E^*$. On le note $E^{**}$.
  \end{definition}

  \begin{example}
    Pour $x \in E$, l'application $\operatorname{ev}_x : \varphi \mapsto \varphi(x)$ est un élément de $E^{**}$.
  \end{example}

  \begin{theorem}
    $x \mapsto \operatorname{ev}_x$ est un isomorphisme entre les espaces $E$ et $E^{**}$.
  \end{theorem}

  \begin{remark}
    Cet isomorphisme est canonique : il ne dépend pas du choix d'une base de $E$.
  \end{remark}

  \begin{corollary}
    Soit $(f_1, \dots, f_n)$ une base de $E^*$. Il existe une unique base $(e_1, \dots, e_n)$ de $E$ telle que, pour tout $i \in \llbracket 1, n \rrbracket$, $e_i^* = f_i$.
  \end{corollary}

  \begin{definition}
    En reprenant les notations précédentes, $(e_1, \dots, e_n)$ est appelée \textbf{base antéduale} de $(f_1, \dots, f_n)$.
  \end{definition}

  \begin{example}
    On suppose $n = 3$. Soient $(e_1, e_2, e_3)$ une base de $E$ et
    \[ f_1^* = 2e_1^* + e_2^* + e_3^*, \, f_2^* = -e_1^* + 2e_3^*, \, f_3^* = e_1^* + 3e_2^* \]
    Alors, $(f_1^*, f_2^*, f_3^*)$ est une base de $E^*$, dont une base antéduale est $(f_1, f_2, f_3)$ où
    \[ f_1 = \frac{1}{13}(6e_1 - 2e_2 + 3e_3), \, f_2 = \frac{1}{13}(-3e_1 - e_2 + 5e_3), \, f_3 = \frac{1}{13}(-2e_1 + 5e_2 - e_3) \]
  \end{example}

  \subsection{Orthogonalité au sens de la dualité}

  \subsubsection{Orthogonal d'une partie, d'une famille}

  \reference[ROM21]{446}

  \begin{definition}
    On dit qu'une forme linéaire $\varphi \in E^*$ et un vecteur $x \in E$ sont orthogonaux si $\varphi(x) = 0$.
  \end{definition}

  \begin{definition}
    \begin{itemize}
      \item L'orthogonal dans $E^*$ d'une partie non vide $X$ de $E$ est l'ensemble
      \[ X^\perp = \{ \varphi \in E^* \mid \forall x \in X, \, \varphi(x) = 0 \} \]
      \item L'orthogonal dans $E$ d'une partie non vide $Y$ de $E^*$ est l'ensemble
      \[ Y^\circ = \{ x \in E \mid \forall \varphi \in Y, \, \varphi(x) = 0 \} \]
    \end{itemize}
  \end{definition}

  \begin{theorem}
    Soient $A$, $B$ des parties non vides de $E$ et $U$, $V$ des parties non vides de $E^*$.
    \begin{enumerate}[label=(\roman*)]
      \item Si $A \subseteq B$, alors $B^\perp \subseteq A^\perp$.
      \item Si $U \subseteq V$, alors $V^\circ \subseteq U^\circ$.
      \item $A \subseteq (A^\perp)^\circ$.
      \item $U \subseteq (U^\circ)^\perp$.
      \item $A^\perp = \operatorname{Vect}(A)^\perp$.
      \item $U^\circ = \operatorname{Vect}(U)^\circ$.
      \item $\{ 0 \}^\perp = E^*, \, E^\perp = \{ 0 \}, \, \{ 0 \}^\circ = E \text{ et }, \, (E^*)^\circ = \{ 0 \}$.
    \end{enumerate}
  \end{theorem}

  \begin{corollary}
    \begin{enumerate}[label=(\roman*)]
      \item Pour tout sous-espace vectoriel $F$ de $E$, on a
      \[ \dim(F) + \dim(F^\perp) = n \]
      \item Pour tout sous-espace vectoriel $G$ de $E^*$, on a
      \[ \dim(G) + \dim(F^\circ) = n \]
      \item Pour tout sous-espace vectoriel $F$ de $E$, et pour tout sous-espace vectoriel $G$ de $E^*$, on a $F = (F^\perp)^\circ$ et $G = (G^\circ)^\perp$.
      \item Pour toute partie $X$ de $E$, on a $(X^\perp)^\circ = \operatorname{Vect}(X)$.
      \item Pour tous sous-espaces vectoriels $F_1$ et $F_2$ de $E$, on a :
      \[ (F_1 + F_2)^\perp = F_1^\perp \, \cap \, F_2^\perp \text{ et } (F_1 \, \cap \, F_2)^\perp = F_1^\perp + F_2^\perp \]
      \item Pour tous sous-espaces vectoriels $G_1$ et $G_2$ de $E^*$, on a :
      \[ (G_1 + G_2)^\circ = G_1^\circ \, \cap \, G_2^\circ \text{ et } (G_1 \, \cap \, G_2)^\circ = G_1^\circ + G_2^\circ \]
    \end{enumerate}
  \end{corollary}

  \begin{corollary}
    Si $(\varphi_i)_{i \in \llbracket 1, p \rrbracket}$ est une famille de formes linéaires sur $E$ de rang $r$, le sous-espace vectoriel $F = \bigcap_{i=1}^p \ker(\varphi_i)$ de $E$ est alors de dimension $n-r$. Réciproquement, si $F$ est un sous-espace vectoriel de $E$ de dimension $m$, il existe alors une famille de formes linéaires $(\varphi_i)_{i \in \llbracket 1, p \rrbracket}$ de rang $r = n-m$ telle que $F = \bigcap_{i=1}^p \ker(\varphi_i)$.
  \end{corollary}

  \subsubsection{Application transposée}

  \reference{452}

  \begin{definition}
    Soient $E$ et $F$ deux espaces vectoriels sur $\mathbb{K}$. Soit $u \in \mathcal{L}(E,F)$. La \textbf{transposée} de $u \in \mathcal{L}(E,F)$ est l'application
    \[
      \tr{u} :
      \begin{array}{ccc}
        F^* &\rightarrow& E^* \\
        \varphi &\mapsto& \varphi \circ u
      \end{array}
    \]
  \end{definition}

  \begin{proposition}
    $u \mapsto \tr{u}$ est linéaire, injective de $\mathcal{L}(E,F)$ dans $\mathcal{L}(F^*,E^*)$.
  \end{proposition}

  \begin{theorem}
    Soient $E$, $F$ et $G$ trois espaces vectoriels sur $\mathbb{K}$. Soient $u \in \mathcal{L}(E,F)$ et $u \in \mathcal{L}(F,G)$. On a :
    \begin{enumerate}[label=(\roman*)]
      \item $\tr{v \circ u} = \tr{u} \circ \tr{v}$.
      \item Pour $F = E$, $\tr{\operatorname{id}_E} = \operatorname{id}_{E^*}$.
      \item Si $u$ est un isomorphisme de $E$ sur $F$, alors $\tr{u}$ est un isomorphisme de $F^*$ sur $E^*$ et $(\tr{u})^{-1} = \tr{(u^{-1})}$.
      \item $\ker(\tr{u}) = (\im(u))^{\perp}$.
      \item $u$ est surjective si et seulement si $\tr{u}$ est injective.
      \item $\im(\tr{u}) = (\ker(u))^{\perp}$.
      \item $u$ est injective si et seulement si $\tr{u}$ est surjective.
      \item Si $E$ et $F$ sont de dimension finie, alors $u$ et $\tr{u}$ ont même rang.
      \item Si $A \in \mathcal{M}_n(\mathbb{K})$ est la matrice de $u$ dans des bases $\mathcal{B}$ et $\mathcal{B}'$, alors $\tr{A}$ est la matrice de $\tr{u}$ dans les bases $\mathcal{B}'^*$ et $\mathcal{B}^*$.
    \end{enumerate}
  \end{theorem}

  \reference[GOU21]{136}

  \begin{corollary}
    Soient $\mathcal{B}$ et $\mathcal{B}'$ deux bases de $E$ et $P$ la matrice de passage de $\mathcal{B}$ à $\mathcal{B}'$. Alors, la matrice de passage de $\mathcal{B}^*$ à $\mathcal{B}'^*$ est
    \[ \tr{P}^{-1} \]
  \end{corollary}

  \begin{proposition}
    Soit $u \in \mathcal{L}(E)$. Alors un sous-espace vectoriel de $E$ est stable par $u$ si et seulement si son orthogonal l'est.
  \end{proposition}

  \reference{176}
  \dev{trigonalisation-simultanee}

  \begin{application}[Trigonalisation simultanée]
    Soit $(u_i)_{i \in I}$ une famille d'endomorphismes de $E$ diagonalisables qui commutent deux-à-deux. Alors, il existe une base commune de trigonalisation.
  \end{application}

  \subsubsection{Lien avec l'orthogonalité au sens euclidien}

  \reference[ROM21]{718}

  \begin{theorem}[de représentation de Riesz]
    Soit $\langle ., . \rangle$ un produit scalaire sur $E$.
    \[ \forall \varphi \in E^*, \, \exists! a \in E \text{ tel que } \forall x \in E, \, \varphi(x) = \langle x, a \rangle \]
  \end{theorem}

  \reference{446}

  Ainsi, si $E$ est muni d'un produit scalaire $\langle ., . \rangle$, on retrouve la notion classique d'orthogonalité euclidienne avec $\varphi : x \mapsto \langle x, a \rangle$.

  \reference[GOU21]{138}

  \begin{example}
    L'application
    \[
    \begin{array}{ccc}
      \mathcal{M}_n(\mathbb{K}) &\rightarrow& \mathcal{M}_n(\mathbb{K})^* \\
      A &\mapsto& (X \mapsto \trace(AX))
    \end{array}
    \]
    est un isomorphisme.
  \end{example}

  \subsection{Applications}

  \subsubsection{Formule de Taylor}

  \reference[ROM21]{442}

  On suppose $\mathbb{K}$ de caractéristique nulle.

  \begin{application}[Formule de Taylor]
    Pour tout $j \in \llbracket 0, n \rrbracket$, on définit :
    \[
    e_j : \begin{array}{ccc}
      \mathbb{K}_n[X] &\rightarrow& \mathbb{K} \\
      P &\mapsto& \frac{P^{(j)}(0)}{j!}
    \end{array}
    \]
    Alors, $(e_i)_{i \in \llbracket 0, n \rrbracket}$ est une base de $K_n[X]^*$, dont la base antéduale est $(X^i)_{i \in \llbracket 0, n \rrbracket}$.
  \end{application}

  \reference[GOU21]{64}

  \begin{corollary}
    On suppose $P \neq 0$. Alors $a \in \mathbb{K}$ est racine d'ordre $h$ de $P$ si et seulement si
    \[ \forall i \in \llbracket 1, h-1 \rrbracket, P^{(i)}(a) = 0 \quad \text{ et } \quad F^{(h)}(a) \neq 0 \]
  \end{corollary}

  \begin{example}
    Le polynôme $P_n = \sum_{i=0}^{n} \frac{1}{i!} X^{i}$ n'a que des racines simples dans $\mathbb{C}$.
  \end{example}

  \begin{remark}
    C'est encore vrai en caractéristique non nulle pour $h = 1$.
  \end{remark}

  \subsubsection{Invariants de similitude}

  \reference[ROM21]{397}

  Soient $E$ un espace vectoriel de dimension finie $n$ et $u \in \mathcal{L}(E)$.

  \begin{definition}
    On dit que $u$ est \textbf{cyclique} s'il existe $x \in E$ tel que $\{ P(u)(x) \mid P \in \mathbb{K}[X] \} = E$.
  \end{definition}

  \begin{proposition}
    $u$ est cyclique si et seulement si $\deg(\pi_u) = n$.
  \end{proposition}

  \begin{definition}
    Soit $P = X^p + a_{p-1} X^{p-1} + \dots + a_0 \in \mathbb{K}[X]$. On appelle \textbf{matrice compagnon} de $P$ la matrice
    \[ \mathcal{C}(P) = \begin{pmatrix} 0 & \dots & \dots & 0 & -a_0 \\ 1 & 0 & \ddots & \vdots & -a_1 \\ 0 & 1 & \ddots & \vdots & \vdots \\ \vdots & \ddots & \ddots & 0 & -a_{p-2} \\ 0 & \dots & 0 & 1 & -a_{p-1} \end{pmatrix} \]
  \end{definition}

  \begin{proposition}
    $u$ est cyclique si et seulement s'il existe une base $\mathcal{B}$ de $E$ telle que $\operatorname{Mat}(u, \mathcal{B}) = \mathcal{C}(\pi_u)$.
  \end{proposition}

  \begin{theorem}
    Il existe $F_1, \dots, F_r$ des sous-espaces vectoriels de $E$ tous stables par $u$ tels que :
    \begin{itemize}
      \item $E = F_1 \oplus \dots \oplus F_r$.
      \item $u_i = u_{|F_i}$ est cyclique pour tout $i$.
      \item Si $P_i = \pi_{u_i}$, on a $P_{i+1} \mid P_i$ pour tout $i$.
    \end{itemize}
    La famille de polynômes $P_1, \dots, P_r$ ne dépend que de $u$ et non du choix de la décomposition. On l'appelle \textbf{suite des invariants de similitude} de $u$.
  \end{theorem}

  \begin{theorem}[Réduction de Frobenius]
    Si $P_1, \dots, P_r$ désigne la suite des invariants de $u$, alors il existe une base $\mathcal{B}$ de $E$ telle que :
    \[ \operatorname{Mat}(u, \mathcal{B}) = \begin{pmatrix} \mathcal{C}(P_1) & & \\ & \ddots & \\ & & \mathcal{C}(P_r) \end{pmatrix} \]
    On a d'ailleurs $P_1 = \pi_u$ et $P_1 \dots P_r = \chi_u$.
  \end{theorem}

  \begin{corollary}
    Deux endomorphismes de $E$ sont semblables si et seulement s'ils ont la même suite d'invariants de similitude.
  \end{corollary}

  \begin{application}
    Pour $n = 2$ ou $3$, deux matrices sont semblables si et seulement si elles ont mêmes polynômes minimal et caractéristique.
  \end{application}

  \begin{application}
    Soit $\mathbb{L}$ une extension de $\mathbb{K}$. Alors, si $A, B \in \mathcal{M}_n(\mathbb{K})$ sont semblables dans $\mathcal{M}_n(\mathbb{L})$, elles le sont aussi dans $\mathcal{M}_n(\mathbb{K})$.
  \end{application}

  \subsubsection{Classification des formes quadratiques}

  \reference{243}

  Soit $q$ une forme quadratique sur $E$.

  \begin{lemma}
    Il existe une base $q$-orthogonale (ie. si $\varphi$ est la forme polaire de $q$, une base $B$ où $\forall e, e' \in B, \, \varphi(e, e') = 0$ si $e \neq e'$).
  \end{lemma}

  \dev{loi-d-inertie-de-sylvester}

  \begin{theorem}[Loi d'inertie de Sylvester]
    \[ \exists p, q \in \mathbb{N} \text{ et } \exists f_1, \dots, f_{p+q} \in E^* \text{ tels que } q = \sum_{i=1}^p |f_i|^2 - \sum_{i=p+1}^{p+q} |f_i|^2 \]
    où les formes linéaires $f_i$ sont linéairement indépendantes et où $p + q \leq n$. De plus, ces entiers ne dépendent que de $q$ et pas de la décomposition choisie.
    \newpar
    Le couple $(p,q)$ est la \textbf{signature} de $q$ et le rang $q$ est égal à $p+q$.
  \end{theorem}

  \begin{example}
    La signature de la forme quadratique $q : (x,y,z) \mapsto x^2 - 2y^2 + xz + yz$ est $(2,1)$, donc son rang est $3$.
  \end{example}
  %</content>
\end{document}

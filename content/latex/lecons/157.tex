\input{../common}
\input{../bibliography}

\begin{document}
  %<*content>
  \lesson{algebra}{157}{Matrices symétriques réelles, matrices hermitiennes.}
  
  Soit $\mathbb{K} = \mathbb{R}$ ou $\mathbb{C}$ et soit $n \geq 1$ un entier.
  
  \subsection{Généralités}
  
  \subsubsection{Espaces \texorpdfstring{$\mathcal{S}_n(\mathbb{R})$}{Sn(R)} et \texorpdfstring{$\mathcal{H}_n(\mathbb{C})$}{Hn(C)}}
  
  \reference[GOU21]{243}
  
  \begin{notation}
    Soit $M \in \mathcal{M}_{n,m}(\mathbb{K})$. On note
    \[
      M^* = \begin{cases}
        \tr{M} &\text{ si } \mathbb{K} = \mathbb{R} \\
        \tr{\overline{M}} &\text{ si } \mathbb{K} = \mathbb{C}
      \end{cases}
    \]
  \end{notation}
  
  \reference{125}
  
  \begin{definition}
    Soit $M \in \mathcal{M}_n(\mathbb{R})$.
    \begin{itemize}
      \item On dit que $M$ est \textbf{symétrique} si $M^* = M$. On note $\mathcal{S}_n(\mathbb{R})$ l'ensemble des matrices symétriques à coefficients réels.
      \item On dit que $M$ est \textbf{antisymétrique} si $M^* = -M$. On note $\mathbb{A}_n(\mathbb{R})$ l'ensemble des matrices antisymétriques à coefficients réels.
    \end{itemize}
  \end{definition}
  
  \reference{240}
  
  \begin{proposition}
    \begin{enumerate}[label=(\roman*)]
      \item $\mathcal{S}_n(\mathbb{R})$ et $\mathcal{A}_n(\mathbb{R})$ sont des sous-espaces vectoriels de $\mathcal{M}_n(\mathbb{R})$ de dimensions respectives $\frac{n(n+1)}{2}$ et $\frac{n(n-1)}{2}$.
      \item $\mathcal{M}_n(\mathbb{R}) = \mathcal{S}_n(\mathbb{R}) \oplus \mathcal{A}_n(\mathbb{R})$.
    \end{enumerate}
  \end{proposition}
  
  \begin{definition}
    Soit $M \in \mathcal{M}_n(\mathbb{C})$. On dit que $M$ est \textbf{hermitienne} si $M^* = M$. On note $\mathcal{H}_n(\mathbb{C})$ l'ensemble des matrices hermitiennes à coefficients complexes.
  \end{definition}
  
  \begin{proposition}
    \[ \forall M \in \mathcal{H}_n(\mathbb{C}) \, \exists!(S, A) \in \mathcal{S}_n(\mathbb{R}) \times \mathcal{A}_n(\mathbb{R}) \text{ tel que } M = S+iA \]
  \end{proposition}
  
  \begin{corollary}
    $\mathcal{H}_n(\mathbb{C})$ est un sous-espace vectoriel du $\mathbb{R}$-espace vectoriel $\mathcal{M}_n(\mathbb{C})$ de dimension $n^2$.
  \end{corollary}
  
  \subsubsection{Positivité}
  
  \reference[ROM21]{735}
  
  \begin{definition}
    Soit $\langle ., . \rangle$ un produit scalaire sur $\mathbb{K}^n$.
    \begin{itemize}
      \item Si $\mathbb{K} = \mathbb{R}$, une matrice symétrique $M \in \mathcal{S}_n(\mathbb{R})$ est dite \textbf{positive} si
      \[ \forall x \in \mathbb{R}^n, \, \langle x, Mx \rangle \geq 0 \]
      et elle est dite \textbf{définie positive} si l'inégalité précédente est stricte pour tout $x \neq 0$. On note respectivement $\mathcal{S}_n^+(\mathbb{R})$ et $\mathcal{S}_n^{++}(\mathbb{R})$ l'ensemble des matrices symétriques positives et définies positives.
      \item Si $\mathbb{K} = \mathbb{C}$, ces définitions sont valables. On note respectivement $\mathcal{H}_n^+(\mathbb{C})$ et $\mathcal{H}_n^{++}(\mathbb{C})$ l'ensemble des matrices hermitiennes positives et définies positives.
    \end{itemize}
  \end{definition}
  
  \begin{proposition}
    Soit $M \in \mathcal{S}_n(\mathbb{R})$. Alors $M \in \mathcal{S}_n^+(\mathbb{R})$ (resp. $\mathcal{S}_n^{++}(\mathbb{R})$) si et seulement si toutes ses valeurs propres sont positives (resp. strictement positives).
  \end{proposition}
  
  \begin{corollary}
    Soit $M \in \mathcal{S}_n(\mathbb{R})$. Alors $M \in \mathcal{S}_n^+(\mathbb{R})$ si et seulement s'il existe $B \in \mathcal{M}_n(\mathbb{R})$ telle que $M = \tr{B}B$.
  \end{corollary}
  
  \begin{theorem}[Critère de Sylvester]
    Une matrice symétrique est définie positive si et seulement si tous ses mineurs principaux sont strictement positifs.
  \end{theorem}
  
  \begin{corollary}
    $\mathcal{S}_n^{++}(\mathbb{R})$ est un ouvert de $\mathcal{M}_n(\mathbb{R})$.
  \end{corollary}
  
  \reference[I-P]{182}
  
  \begin{lemma}
    Soit $S \in \mathcal{S}_n(\mathbb{R})$. Alors,
    \[ \VERT S \VERT_2 = \rho(S) \]
    où $\rho$ est l'application qui a une matrice y associe son rayon spectral.
  \end{lemma}
  
  \dev{homeomorphisme-de-l-exponentielle}
  
  \begin{theorem}
    L'application $\exp : \mathcal{S}_n(\mathbb{R}) \rightarrow \mathcal{S}^{++}_n(\mathbb{R})$ est un homéomorphisme.
  \end{theorem}
  
  \subsubsection{Lien avec l'algèbre bilinéaire}
  
  \reference[GOU21]{239}
  
  Soit $E$ un espace vectoriel sur $\mathbb{K}$ de dimension $n$.
  
  \begin{definition}
    Soit $\varphi : E \times E \rightarrow \mathbb{K}$ une application.
    \begin{itemize}
      \item On dit que $\varphi$ est une \textbf{forme bilinéaire} sur $E$ si pour tout $x \in E$, $y \mapsto \varphi(x, y)$ et pour tout $y \in E$, $x \mapsto \varphi(x, y)$ sont linéaires.
      \item Si $\mathbb{K} = \mathbb{C}$, on dit que $\varphi$ est une \textbf{forme sesquilinéaire} sur $E$ si pour tout $x \in E$, $y \mapsto \varphi(x, y)$ est linéaire et pour tout $y \in E$, $x \mapsto \varphi(x, y)$ est antilinéaire (ie. $\forall x, y, z \in E$, $\forall \lambda \in \mathbb{C}$, $\varphi(x+y, z) = \varphi(x,z) + \varphi(y,z)$ et $\varphi(\lambda x, z) = \overline{\lambda} \varphi(x,z)$).
    \end{itemize}
  \end{definition}
  
  \begin{example}
    \begin{itemize}
      \item Toute forme sesquilinéaire sur $E$ est une forme bilinéaire lorsque $E$ est considéré comme un espace vectoriel sur $\mathbb{R}$.
      \item L'application
      \[
        \begin{array}{ccc}
          \mathcal{C}([0,1], \mathbb{C})^2 &\rightarrow& \mathbb{C} \\
          (f,g) &\mapsto& \int_0^1 \overline{f(t)} g(t) \, \mathrm{d}t
        \end{array}
      \]
      est une forme sesquilinéaire sur $\mathcal{C}([0,1], \mathbb{C})$.
    \end{itemize}
  \end{example}
  
  \begin{definition}
    On définit la matrice d'une forme bilinéaire (ou sesquilinéaire) $\varphi$ dans une base $(e_1, \dots, e_n)$ de $E$ par
    \[ (\varphi(e_i, e_j))_{i, j \in \llbracket 1, n \rrbracket} \in \mathcal{M}_n(\mathbb{K}) \]
  \end{definition}
  
  \begin{remark}
    Soit $\mathcal{B} = (e_1, \dots, e_n)$ une base de $E$. Soient $x = \sum_{i=1}^{n} x_i e_i \in E$ et $y = \sum_{i=1}^{n} y_i e_i \in E$. Soit $\varphi$ une forme bilinéaire ou sesquilinéaire, dont on note $M$ sa matrice dans le base $\mathcal{B}$. On a :
    \[ \varphi(x,y) = X^* M Y \text{, où } X = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \text{ et } Y = \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix} \]
  \end{remark}
  
  \begin{definition}
    Soit $\varphi$ une forme bilinéaire sur $E$. On dit que :
    \begin{itemize}
      \item $\varphi$ est \textbf{symétrique} si $\forall x, y \in E, \, \varphi(x,y) = \varphi(y,x)$.
      \item $\varphi$ est \textbf{antisymétrique} si $\forall x, y \in E, \, \varphi(x,y) = -\varphi(y,x)$.
      \item Si $\mathbb{K} = \mathbb{C}$, on dit que $\varphi$ est \textbf{hermitienne} si $\forall x, y \in E, \, \varphi(x,y) = \overline{\varphi(y,x)}$.
    \end{itemize}
  \end{definition}
  
  \begin{proposition}
    \begin{enumerate}[label=(\roman*)]
      \item Une forme bilinéaire est symétrique (resp. antisymétrique) si et seulement si sa matrice dans une base est symétrique (resp. antisymétrique).
      \item Une forme sesquilinéaire est hermitienne si et seulement si sa matrice dans une base est hermitienne.
    \end{enumerate}
  \end{proposition}
  
  \begin{definition}
    On appelle \textbf{forme quadratique} sur $E$ toute application $q$ de la forme
    \[
    q :
    \begin{array}{ccc}
      E &\rightarrow& \mathbb{K} \\
      x &\mapsto& \varphi(x, x)
    \end{array}
    \]
    où $\varphi$ est une forme bilinéaire symétrique sur $E$.
  \end{definition}
  
  \begin{proposition}
    Soit $q$ une forme quadratique sur $E$. Il existe une unique forme bilinéaire symétrique $\varphi$ telle que pour tout $x \in E$, $q(x)=\varphi(x,x)$.
    \newpar
    $\varphi$ est alors la \textbf{forme polaire} de $q$, et on a
    \[ \forall x, y \in E, \, \varphi(x,y) = \frac{1}{2} (q(x+y) - q(x) - q(y)) \]
  \end{proposition}
  
  \begin{example}
    La matrice symétrique
    \[
      \begin{pmatrix}
        3 & 1 & -\frac{3}{2} \\
        1 & 0 & 0 \\
        -\frac{3}{2} & 0 & 0
      \end{pmatrix}
    \]
    définit la forme quadratique $q : (x, y, z) \mapsto 3x^2 + y^2 + 2xy - 3xz$.
  \end{example}
  
  \subsection{Réductions, décompositions}
  
  \subsubsection{Réductions}
  
  \reference{254}
  
  \begin{definition}
    Soit $M \in \mathcal{M}_n(\mathbb{K})$ telle que $\tr{M}M = I_n$.
    \begin{itemize}
      \item Si $\mathbb{K} = \mathbb{R}$, on dit que $M$ est \textbf{orthogonale}. On note $\mathcal{O}_n(\mathbb{R})$ l'ensemble des matrices orthogonales à coefficients réels.
      \item Si $\mathbb{K} = \mathbb{C}$, on dit que $M$ est \textbf{unitaire}. On note $\mathcal{U}_n(\mathbb{C})$ l'ensemble des matrices unitaires à coefficients complexes.
    \end{itemize}
  \end{definition}
  
  \begin{theorem}[Spectral]
    \label{157-1}
    Soit $M \in \mathcal{S}_n(\mathbb{R})$ (resp. $M \in \mathcal{H}_n(\mathbb{C})$). Alors il existe $C \in \mathcal{O}_n(\mathbb{R})$ (resp. $C \in \mathcal{U}_n(\mathbb{C})$) telle que
    \[ C^{-1}MC = C^* M C = D \]
    où $D$ est une matrice diagonale réelle.
  \end{theorem}
  
  \begin{remark}
    En reprenant les notations précédentes, cela revient à dire qu'un endomorphisme ayant $M$ pour matrice dans une base est diagonalisable dans une base orthonormée.
  \end{remark}
  
  \begin{corollary}
    Soient $M, N \in \mathcal{S}_n(\mathbb{R})$ (resp. $M \in \mathcal{H}_n(\mathbb{C})$) définies positives. Alors il existe $C$ inversible telle que
    \[ C^*MC = I_n \text{ et } C^*NC = D \]
    où $D$ est une matrice diagonale réelle.
  \end{corollary}
  
  \reference{283}
  
  \begin{application}
    \[ \forall A, B \in \mathcal{H}_n^{++}(\mathbb{C}), \, \det(A+B)^{\frac{1}{n}} \geq \det(A)^{\frac{1}{n}} + \det(B)^{\frac{1}{n}} \]
  \end{application}
  
  \reference[ROM21]{738}
  
  Comme application du \cref{157-1}, on a les résultats suivants.
  
  \begin{application}[Norme euclidienne sur $\mathcal{S}_n(\mathbb{R})$]
    Soit $A = (a_{i,j})_{i,j \in \llbracket 1, n \rrbracket} \in \mathcal{S}_n(\mathbb{R})$ de valeurs propres $\lambda_1, \dots, \lambda_n \in \mathbb{R}$. On a
    \[ \sum_{i,j = 1}^n a_{i,j}^2 = \sum_{i=1}^n \lambda_i^2 \]
  \end{application}
  
  \begin{application}[Diagonalisation simultanée]
    Soit $(A_i)_{i \in I}$ une famille de matrices symétriques. Alors, il existe $P \in \mathcal{O}_n(\mathbb{R})$ telle que pour tout $i \in I$, la matrice $\tr{P} A_i P$ est diagonale si et seulement si $A_i A_j = A_j A_i$ pour tout $i \neq j$.
  \end{application}
  
  \reference[C-G]{376}
  
  \begin{application}[Racine carrée dans $\mathcal{S}_n^{++}(\mathbb{R})$ et $\mathcal{H}_n^{++}(\mathbb{C})$]
    \[ \forall A \in \mathcal{S}_n^{++}(\mathbb{R}) \, \exists! B \in \mathcal{S}_n^{++}(\mathbb{R}) \text{ telle que } B^2 = A \]
    et on a le même résultat en remplaçant $\mathcal{S}_n^{++}(\mathbb{R})$ par $\mathcal{H}_n^{++}(\mathbb{C})$.
  \end{application}
  
  \reference{299}
  
  \begin{theorem}[Loi d'inertie de Sylvester]
    Soit $A \in \mathcal{S}_n(\mathbb{R})$. Alors, il existe $P \in \mathrm{GL}_n(\mathbb{R})$ et un unique couple d'entiers $(p,q)$ tels que
    \[ \tr{P}AP = \begin{pmatrix} I_p & 0 & 0 \\ 0 & -I_q & 0 \\ 0 & 0 & 0 \end{pmatrix} \]
  \end{theorem}
  
  \begin{definition}
    Le couple $(p,q)$ précédent est la \textbf{signature} de $A$.
  \end{definition}
  
  \begin{proposition}
    Soit $q$ une forme quadratique de forme polaire sur $\mathbb{R}^n$. Alors les conditions suivantes sont équivalentes :
    \begin{enumerate}[label=(\roman*)]
      \item $\varphi$ est un produit scalaire.
      \item $q$ est de signature $(n,0)$.
      \item La matrice de $\varphi$ dans une base de $\mathbb{R}^n$ est de la forme $S = \tr{P}P$ avec $P \in \mathrm{GL}_n(\mathbb{R})$.
      \item La matrice de $\varphi$ dans une base de $\mathbb{R}^n$ est de la forme $S = \tr{P}P$ avec $P \in \mathrm{GL}_n(\mathbb{R})$ triangulaire supérieure.
    \end{enumerate}
  \end{proposition}
  
  \reference{270}
  
  \begin{remark}
    Soit $M \in \mathcal{S}_n(\mathbb{R})$ de signature $(p,q)$. Alors, $p$ (resp. $q$) est le nombre de valeurs propres de $M$ strictement positives (resp. strictement négatives).
  \end{remark}
  
  \begin{corollary}
    Soient $A, B \in \mathcal{S}_n(\mathbb{R})$. Alors $A$ et $B$ sont congruentes si et seulement si elles sont de même signature.
  \end{corollary}
  
  \reference{348}
  
  \begin{application}
    \[
      \{ P P^* \mid P \in \mathrm{GL}_n(\mathbb{K}) \} =
      \begin{cases}
        \mathcal{S}_n^{++}(\mathbb{R}) &\text{ si } \mathbb{K} = \mathbb{R} \\
        \mathcal{H}_n^{++}(\mathbb{C}) &\text{ si } \mathbb{K} = \mathbb{C}
      \end{cases}
    \]
  \end{application}
  
  \subsubsection{Décompositions}
  
  \reference{376}
  \dev{decomposition-polaire}
  
  \begin{application}[Décomposition polaire]
    L'application
    \[ \mu :
    \begin{array}{ccc}
      \mathcal{O}_n(\mathbb{R}) \times \mathcal{S}_n^{++}(\mathbb{R}) &\rightarrow& \mathrm{GL}_n(\mathbb{R}) \\
      (O, S) &\mapsto& OS
    \end{array}
    \]
    est un homéomorphisme.
  \end{application}
  
  \begin{corollary}
    Tout sous-groupe compact de $\mathrm{GL}_n(\mathbb{R})$ qui contient $\mathcal{O}_n(\mathbb{R})$ est $\mathcal{O}_n(\mathbb{R})$.
  \end{corollary}
  
  \reference[ROM21]{690}
  
  \begin{definition}
    Les \textbf{sous-matrices principales} d'une matrice $(a_{i,j})_{i,j \in \llbracket 1, n \rrbracket} \in \mathcal{M}_n(\mathbb{K})$ sont les matrices $A_k = (a_{i,j})_{i,j \in \llbracket 1, k \rrbracket} \in \mathcal{M}_k(\mathbb{K})$ où $k \in \llbracket 1, n \rrbracket$. Les \textbf{déterminants principaux} sont les déterminants des matrices $A_k$, pour $k \in \llbracket 1, n \rrbracket$.
  \end{definition}
  
  \begin{theorem}[Décomposition lower-upper]
    \label{157-2}
    Soit $A \in \mathrm{GL}_n(\mathbb{K})$. Alors, $A$ admet une décomposition
    \[ A = LU \]
    (où $L$ est une matrice triangulaire inférieure à diagonale unité et $U$ une matrice triangulaire supérieure) si et seulement si tous les déterminants principaux de $A$ sont non nuls. Dans ce cas, une telle décomposition est unique.
  \end{theorem}
  
  \begin{corollary}
    Soit $A \in \mathrm{GL}_n(\mathbb{K}) \, \cap \, \mathcal{S}_n(\mathbb{K})$. Alors, on a l'unique décomposition de $A$ :
    \[ A = LD\tr{L} \]
    où $L$ est une matrice triangulaire inférieure et $D$ une matrice diagonale.
  \end{corollary}
  
  \begin{application}[Décomposition de Cholesky]
    Soit $A \in \mathcal{M}_n(\mathbb{R})$. Alors, $A \in \mathcal{S}_n^{++}(\mathbb{R})$ si et seulement s'il existe $B \in \mathrm{GL}_n(\mathbb{R})$ triangulaire inférieure telle que $A = B\tr{B}$. De plus, une telle décomposition est unique si on impose la positivité des coefficients diagonaux de $B$.
  \end{application}
  
  \reference[GRI]{368}
  
  \begin{example}
    On a la décomposition de Cholesky :
    \[ \begin{pmatrix} 1 & 2 \\ 2 & 5 \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 2 & 1 \end{pmatrix} \begin{pmatrix} 1 & 2 \\ 0 & 1 \end{pmatrix} \]
  \end{example}
  
  \subsection{Applications}
  
  \subsubsection{Géométrie différentielle}
  
  \reference[ROU]{209}
  
  \begin{lemma}
    Soit $A_0 \in \mathcal{S}_n(\mathbb{R})$ inversible. Alors il existe un voisinage $V$ de $A_0$ dans $\mathcal{S}_n(\mathbb{R})$ et une application $\psi : V \rightarrow \mathrm{GL}_n(\mathbb{R})$ de classe $\mathcal{C}^1$ telle que
    \[ \forall A \in V, \, A = \tr \psi(A) A_0 \psi(A) \]
  \end{lemma}
  
  \reference{354}
  
  \begin{lemma}[Morse]
    Soit $f : U \rightarrow \mathbb{R}$ une fonction de classe $\mathcal{C}^3$ (où $U$ désigne un ouvert de $\mathbb{R}^n$ contenant l'origine). On suppose :
    \begin{itemize}
      \item $\mathrm{d} f_0 = 0$.
      \item La matrice symétrique $\mathrm{Hess} (f)_0$ est inversible.
      \item La signature de $\mathrm{Hess}(f)_0$ est $(p, n-p)$.
    \end{itemize}
    Alors il existe un difféomorphisme $\phi = (\phi_1, \dots, \phi_n)$ de classe $\mathcal{C}^1$ entre deux voisinage de l'origine de $\mathbb{R}^n$ $V \subseteq U$ et $W$ tel que $\varphi(0) = 0$ et
    \[ \forall x \in U, \, f(x) - f(0) = \sum_{k=1}^p \phi_k^2(x) - \sum_{k=p+1}^n \phi_k^2(x) \]
  \end{lemma}
  
  \reference{341}
  
  \begin{application}
    Soit $S$ la surface d'équation $z = f(x, y)$ où $f$ est de classe $\mathcal{C}^3$ au voisinage de l'origine. On suppose la forme quadratique $\mathrm{d}^2 f_0$ non dégénérée. Alors, en notant $P$ le plan tangent à $S$ en $0$ :
    \begin{enumerate}[label=(\roman*)]
      \item Si $\mathrm{d}^2 f_0$ est de signature $(2, 0)$, alors $S$ est au-dessus de $P$ au voisinage de $0$.
      \item Si $\mathrm{d}^2 f_0$ est de signature $(0, 2)$, alors $S$ est en-dessous de $P$ au voisinage de $0$.
      \item Si $\mathrm{d}^2 f_0$ est de signature $(1, 1)$, alors $S$ traverse $P$ selon une courbe admettant un point double en $(0, f(0))$.
    \end{enumerate}
  \end{application}
  
  \subsubsection{Résolution de systèmes linéaires}
  
  \reference[C-G]{257}
  
  \begin{proposition}
    Soit $A \in \mathrm{GL}_n(\mathbb{K})$ vérifiant les hypothèses du \cref{157-2}. On définit la suite $(A_k)$ où $A_0 = A$ et $\forall k \in \mathbb{N}$, $A_{k+1}$ est la matrice obtenue à partir de $A_k$ à l'aide du pivot de Gauss sur la $(k+1)$-ième colonne. Alors, $A_{n-1}$ est la matrice $U$ de la décomposition $A = LU$ du \cref{157-2}.
  \end{proposition}
  
  \begin{remark}
    Pour résoudre un système linéaire $AX = Y$, on se ramène à $A = LU$ en $O \left( \frac{2}{3}n^3 \right)$. Puis, on résout deux systèmes triangulaires ``en cascade'' :
    \[ LX' = Y \text{ puis } UX = X' \]
    ceux-ci demandant chacun $O(2n^2)$ opérations.
  \end{remark}
  %</content>
\end{document}

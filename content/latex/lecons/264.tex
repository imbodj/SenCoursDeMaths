\input{../common}
\input{../bibliography}

\begin{document}
  %<*content>
  \lesson{analysis}{264}{Variables aléatoires discrètes. Exemples et applications.}

  Soient $(\Omega, \mathcal{A}, \mathbb{P})$ un espace probabilisé et $X : \Omega \rightarrow \mathbb{R}$ une variable aléatoire réelle. On munit $\mathbb{R}$ de sa tribu borélienne $\mathcal{B}(\mathbb{R})$.

  \subsection{Généralités}

  \subsubsection{Définitions}

  \reference[G-K]{335}

  \begin{definition}
    \begin{itemize}
      \item On dit qu'une loi $\mu$ est \textbf{discrète} s'il existe un ensemble $D$ fini tel que $\mu(D) = 1$.
      \item On dit que la variable aléatoire $X$ est discrète si sa loi $\mathbb{P}_X$ est discrète.
    \end{itemize}
  \end{definition}

  \reference[GOU21]{335}

  \begin{remark}
    Cela revient à dire que $X(\Omega)$ est fini ou est dénombrable.
  \end{remark}

  \begin{example}
    On pose $\Omega = \{ (\omega_n) \in \mathbb{R}^n \mid \omega_n \in \{ 0,1 \} \, \forall n \in \mathbb{N} \}$ et $X : (\omega_n) \mapsto \inf \{ n \in \mathbb{N} \mid \omega_n = 0 \}$. Alors $X$ est une variable aléatoire discrète, à valeurs dans $\mathbb{N} \cup \{ +\infty \}$.
  \end{example}

  \reference[G-K]{131}

  \begin{proposition}
    Si $X$ est une variable aléatoire discrète à valeurs dans un ensemble dénombrable $D$, alors :
    \begin{enumerate}[label=(\roman*)]
      \item $\forall A \in \mathcal{B}(\mathbb{R}), \mathbb{P}_X(A) = \sum_{i \in D \cap A} \mathbb{P}(X=i)$.
      \item $\mathbb{P}_X = \sum_{i \in D} \mathbb{P}(X=i) \delta_i$ où les $\delta_i$ sont des masses de Dirac (voir \cref{264-1}).
    \end{enumerate}
  \end{proposition}

  \begin{remark}
    Si $D$ est un ensemble fini ou dénombrable et $(p_i)_{i \in D}$ est une famille de réels positifs de somme égale à $1$, alors en posant $\Omega = D$, $\mathcal{A} = \mathcal{P}(D)$, $X : \omega \mapsto \omega$ et $\mathbb{P} = \sum_{i \in D} \mathbb{P}(X = i) \delta_i$, on a construit une variable aléatoire discrète $X$ sur $(\Omega, \mathcal{A}, \mathbb{P})$.
  \end{remark}

  \subsubsection{Lois discrètes usuelles}

  \reference{137}

  \begin{definition}
    Si $A \subseteq \Omega$, l'application $\mathbb{1}_A$, appelée \textbf{indicatrice} de $A$ est définie sur $\Omega$ par
    \[
      \mathbb{1}_A :
      \begin{array}{ccc}
        \Omega &\rightarrow& \{ 0; 1 \} \\
        x &\mapsto& \begin{cases}
          1 \text{ si } x \in A \\
          0 \text{ sinon}
        \end{cases}
      \end{array}
    \]
  \end{definition}

  \begin{example}[Mesure de Dirac]
    \label{264-1}
    Si $x \in \Omega$, on pose $\delta_x : A \mapsto \mathbb{1}_A(x)$. C'est une loi discrète sur $\mathcal{P}(\Omega)$.
  \end{example}

  \begin{example}[Loi uniforme]
    Soit $E \subseteq \Omega$ fini. On appelle loi uniforme sur $E$ la loi discrète définie sur $\mathcal{P}(\Omega)$ par
    \[
    \begin{array}{ccc}
      \mathcal{P}(\Omega) &\rightarrow& \llbracket 0, 1 \rrbracket \\
      A &\mapsto& \frac{\vert A \, \cap \, E \vert}{\vert E \vert}
    \end{array}
    \]
  \end{example}

  \begin{remark}
    Il s'agit du nombre de cas favorables sur le nombre de cas possibles. Ainsi, $X$ suit la loi uniforme sur $E$ si on a $\forall x \in E, \, \mathbb{P}(X=x) = \frac{1}{\vert E \vert}$ et $\forall x \notin E, \, \mathbb{P}(X=x) = 0$.
    \newpar
    C'est, par exemple, la loi suivie par une variable aléatoire représentant le lancer d'un dé non truqué avec $E = \llbracket 1, 6 \rrbracket$.
  \end{remark}

  \begin{example}[Loi de Bernoulli]
    $X$ suit une loi de Bernoulli de paramètre $p \in [0,1]$, notée $\mathcal{B}(p)$, si $\mathbb{P}(X=1) = p$ et $\mathbb{P}(X=0)=1-p$. Dans ce cas, $X$ est bien une loi discrète et on a
    \[ \mathbb{P}_X = (1-p) \delta_0 + p \delta_1 \]
  \end{example}

  \begin{example}[Loi binomiale]
    $X$ suit une loi de binomiale de paramètres $n \in \mathbb{N}$ et $p \in [0,1]$, notée $\mathcal{B}(n, p)$, si $X$ est la somme de $n$ variables aléatoires indépendantes qui suivent des lois de Bernoulli de paramètre $p$. Dans ce cas, $X$ est bien une loi discrète et on a
    \[ \forall k \in \mathbb{N}, \, \mathbb{P}(X = k) = \binom{n}{k} p^k (1-p)^{n-k} \]
  \end{example}

  \begin{remark}
    Il s'agit du nombre de succès pour $n$ tentatives.
    \newpar
    C'est, par exemple, la loi suivie par une variable aléatoire représentant le nombre de ``Pile'' obtenus lors d'un lancer de pièce équilibrée.
  \end{remark}

  \begin{example}[Loi géométrique]
    $X$ suit une loi géométrique de paramètre $p \in ]0,1]$, notée $\mathcal{G}(p)$, si l'on a
    \[ \forall k \in \mathbb{N}^{*}, \, \mathbb{P}(X = k) = p(1-p)^{k-1} \]
  \end{example}

  \begin{remark}
    Il s'agit d'une succession de $k-1$ échecs consécutifs suivie d'un succès.
    \newpar
    C'est, par exemple, la loi suivie par une variable aléatoire représentant le nombre de lancers effectués avant d'obtenir ``Pile'' lors d'un lancer de pièce équilibrée.
  \end{remark}

  \begin{example}[Loi de Poisson]
    $X$ suit une loi de Poisson de paramètre $\lambda > 0$, notée $\mathcal{P}(\lambda)$, si l'on a
    \[ \forall k \in \mathbb{N}^{*}, \, \mathbb{P}(X = k) = e^{-\lambda} \frac{\lambda^k}{k!} \]
  \end{example}

  \reference{298}

  \begin{remark}
    Cette loi est une bonne modélisation pour le nombre de fois où un événement rare survient (par exemple, un tremblement de terre).
  \end{remark}

  \subsection{Propriétés spécifiques aux variables aléatoires discrètes}

  \subsubsection{Indépendance}

  \reference{128}

  \begin{definition}
    On dit que des variables aléatoires $X_1, \dots X_n$, sont \textbf{indépendantes} si
    \[ \mathbb{P}_{(X_1, \dots, X_n)} = \bigotimes_{i=1}^n \mathbb{P}_{X_i} \]
  \end{definition}

  \reference{238}

  \begin{example}
    Si $X_1$ et $X_2$ sont des variables aléatoires indépendantes suivant des lois de Poisson de paramètres respectifs $\lambda$ et $\mu$, alors $X_1 + X_2$ suit une loi de Poisson de paramètre $\lambda + \mu$.
  \end{example}

  \begin{cexample}
    Soient $X_1$ et $X_2$ deux variables aléatoires indépendantes telles que
    \[ \forall i \in \llbracket 1, 2 \rrbracket, \, \mathbb{P}(X_i = 1) = \mathbb{P}(X_i = -1) = \frac{1}{2} \]
    On pose $X_3 = X_1 X_2$. Alors, $X_2$ et $X_3$ sont indépendantes, $X_1$ et $X_3$ aussi, mais $X_1$, $X_2$ et $X_3$ ne le sont pas.
  \end{cexample}

  \reference[GOU21]{337}

  \begin{proposition}
    Des variables aléatoires discrètes $X_1, \dots, X_n$ sont indépendantes si et seulement si
    \[ \forall j \in \llbracket, 1, n \rrbracket, \, \forall x_j \in X_j(\Omega), \, \mathbb{P}(X_1 = x_1, \dots, X_n = x_n) = \prod_{j = 1}^n \mathbb{P}(X = x_i) \]
  \end{proposition}

  \begin{proposition}
    Soient $X_1, \dots, X_n$ des variables aléatoires discrètes définies sur $(\Omega, \mathcal{A}, \mathbb{P})$, $f : X_1(\Omega) \times \dots \times X_m(\Omega) \rightarrow F$ et $g : X_{m+1}(\Omega) \times \dots \times X_n(\Omega) \rightarrow F'$ deux fonctions. Si $X_1, \dots, X_n$ sont indépendantes, alors il en est de même de $f(X_1, \dots, X_m)$ et $g(X_{m+1}, \dots, X_n)$.
  \end{proposition}

  \subsubsection{Espérance}

  \reference[G-K]{159}

  \begin{definition}
    \begin{itemize}
      \item On note $\mathcal{L}_1(\Omega, \mathcal{A}, \mathbb{P})$ (ou simplement $\mathcal{L}_1(\Omega)$ voire $\mathcal{L}_1$ s'il n'y a pas d'ambiguïté) l'espace des variables aléatoires intégrables sur $(\Omega, \mathcal{A}, \mathbb{P})$.
      \item Si $X \in \mathcal{L}_1$, on peut définir son \textbf{espérance}
      \[ \mathbb{E}(X) = \int_\Omega X(\omega) \, \mathrm{d}\mathbb{P}(\omega) \]
    \end{itemize}
  \end{definition}

  \reference{164}

  \begin{theorem}[Transfert]
    Si $X$ est une variable aléatoire dont la loi $\mathbb{P}_X$ admet une densité $f$ par rapport à $\mathbb{P}$ et si $g$ est une fonction mesurable, alors
    \[ g(X) \in \mathcal{L}_1 \iff \int_{\mathbb{R}} \vert g(x) \vert f(x) \, \mathrm{d}\mathbb{P}(x) < +\infty \]
    et dans ce cas,
    \[ \mathbb{E}(g(X)) = \int_{\mathbb{R}} g(x) f(x) \, \mathrm{d}\mathbb{P}(x) \]
  \end{theorem}

  \begin{corollary}
    Soit $g$ une fonction mesurable. Si $X$ est une variable aléatoire discrète telle que $X(\Omega) = D$, alors
    \[ g(X) \in \mathcal{L}_1 \iff \sum_{i \in D} \vert g(i) \vert \mathbb{P}(X = i) < +\infty \]
    et dans ce cas,
    \[ \mathbb{E}(g(X)) = \sum_{i \in D} g(i) \mathbb{P}(X = i) \]
  \end{corollary}

  \begin{remark}
    En reprenant les notations précédentes, et avec $g : x \mapsto x$, on a
    \[ X \in \mathcal{L}_1 \iff \sum_{i \in D} \vert i \vert \mathbb{P}(X = i) < +\infty \]
    et dans ce cas,
    \[ \mathbb{E}(X) = \sum_{i \in D} i \mathbb{P}(X = i) \]
  \end{remark}

  \reference{187}

  \begin{example}
    \begin{itemize}
      \item $\mathbb{E}(\mathbb{1}_A) = \mathbb{P}(A)$.
      \item $X \sim \mathcal{B}(n, p) \implies \mathbb{E}(X) = np$.
      \item $X \sim \mathcal{G}(p) \implies \mathbb{E}(X) = \frac{1}{p}$.
      \item $X \sim \mathcal{P}(\lambda) \implies \mathbb{E}(X) = \lambda$.
    \end{itemize}
  \end{example}

  \reference{171}

  \begin{proposition}
    Si $X$ est à valeurs dans $(\mathbb{N}, \mathcal{P}(\mathbb{N}))$, alors $\mathbb{E}(X) = \sum_{k=0}^{+\infty} \mathbb{P}(X > k)$.
  \end{proposition}

  \subsubsection{Fonctions génératrices}

  On suppose dans cette sous-section que $X$ est à valeurs dans $(\mathbb{N}, \mathcal{P}(\mathbb{N}))$.

  \reference{235}

  \begin{definition}
    On appelle \textbf{fonction génératrice} de $X$ la fonction
    \[
      G_X :
      \begin{array}{ccc}
        [-1,1] &\rightarrow& \mathbb{R} \\
        z &\mapsto& \sum_{k=0}^{+\infty} \mathbb{P}(X=k) z^k
      \end{array}
    \]
  \end{definition}

  \begin{example}
    \begin{itemize}
      \item $X \sim \mathcal{B}(p) \implies \forall s \in [-1,1], \, G_X(s) = (1-p) + ps$.
      \item $X \sim \mathcal{B}(n, p) \implies \forall s \in [-1,1], \, G_X(s) = ((1-p) + ps)^n$.
      \item $X \sim \mathcal{G}(p) \implies \forall s \in [-1,1], \, G_X(s) = \frac{ps}{1-(1-p)s}$.
      \item $X \sim \mathcal{P}(\lambda) \implies \forall s \in [-1,1], \, G_X(s) = e^{-\lambda (1-s)}$.
    \end{itemize}
  \end{example}

  \begin{theorem}
    Soient $X_1$ et $X_2$ deux variables aléatoires indépendantes et $\mathcal{L}_1$. Alors,
    \[ \mathbb{E}(X_1 X_2) = \mathbb{E}(X_1) \mathbb{E}(X_2) \]
  \end{theorem}

  \begin{corollary}
    Soient $X_1$ et $X_2$ deux variables aléatoires indépendantes et à valeurs dans $\mathbb{N}$. Alors,
    \[ G_{X_1 X_2} = G_{X_1} + G_{X_2} \]
  \end{corollary}

  \begin{theorem}
    Sur $[0,1]$, la fonction $G_X$ est infiniment dérivable et ses dérivées sont toutes positives, avec
    \[ G_X^{(n)}(s) = \mathbb{E}(X(X-1) \dots (X-n+1)s^{X-n}) \]
    En particulier,
    \[ \mathbb{P}(X=n) = \frac{G_X^{(n)}(0)}{n!} \]
    ce qui montre que la fonction génératrice caractérise la loi.
  \end{theorem}

  \reference[GOU21]{346}

  \begin{example}
    Si $X_1 \sim \mathcal{B}(n, p)$ et $X_2 \sim \mathcal{B}(m, p)$ sont indépendantes, alors $X_1 + X_2 \sim \mathcal{B}(n + m, p)$.
  \end{example}

  \reference[G-K]{238}

  \begin{theorem}
    $X \in \mathcal{L}_1$ si et seulement si $G_X$ admet une dérivée à gauche en $1$. Dans ce cas, $G'_X(1) = \mathbb{E}(X)$.
  \end{theorem}

  \subsection{Application en analyse réelle}

  \reference{171}

  \begin{definition}
    On dit que $X$ \textbf{admet un moment d'ordre $2$} si elle est de carré intégrable, ie. $X^2 \in \mathcal{L}_1$. On note $\mathcal{L}_1(\Omega, \mathcal{A}, \mathbb{P})$ (ou simplement $\mathcal{L}_1(\Omega)$ voire $\mathcal{L}_1$ s'il n'y a pas d'ambiguïté) l'espace des variables aléatoires de carré intégrable.
  \end{definition}

  \begin{proposition}
    \[ X_1, X_2 \in \mathcal{L}_2 \implies X_1 X_2 \in \mathcal{L}_1 \]
    En particulier, $X_1 \in \mathcal{L}_2 \implies X_1 \in \mathcal{L}_1$.
  \end{proposition}

  \begin{definition}
    Soient $X_1$ et $X_2$ deux variables aléatoires admettant chacune un moment d'ordre $2$.
    \begin{itemize}
      \item On appelle \textbf{covariance} du couple $(X_1, X_2)$ le réel
      \[ \operatorname{Covar}(X_1, X_2) = \mathbb{E}((X_1 - \mathbb{E}(X_1))(X_2 - \mathbb{E}(X_2))) \]
      \item On appelle \textbf{variance} de $X_1$ le réel positif
      \[ \operatorname{Var}(X_1) = \operatorname{Covar}(X_1, X_1) = \mathbb{E}(X_1 - \mathbb{E}(X_1))^2 = \mathbb{E}(X_1^2) - (\mathbb{E}(X_1))^2  \]
    \end{itemize}
  \end{definition}

  \reference[GOU21]{346}

  \begin{proposition}
    Si $X$ est à valeurs dans $\mathbb{N}$, alors $X \in \mathcal{L}_2$ si et seulement si $G_X \in \mathcal{C}^2([0,1])$, et dans ce cas,
    \[ \operatorname{Var}(X) = G''_X(1) + G'_X(1) - G'_X(1)^2 \]
  \end{proposition}

  \reference[G-K]{186}

  \begin{example}
    \begin{itemize}
      \item $\operatorname{Var}(\mathbb{1}_A) = \mathbb{P}(A)$.
      \item $X \sim \mathcal{B}(n, p) \implies \operatorname{Var}(X) = np (1-p)$.
      \item $X \sim \mathcal{G}(p) \implies \operatorname{Var}(X) = \frac{1-p}{p^2}$.
      \item $X \sim \mathcal{P}(\lambda) \implies \operatorname{Var}(X) = \lambda$.
    \end{itemize}
  \end{example}

  \reference{177}

  \begin{proposition}[Inégalité de Bienaymé-Tchebychev]
    On suppose $X \in \mathcal{L}_2$. Alors,
    \[ \forall a > 0, \, \mathbb{P}(\vert X - \mathbb{E}(X) \vert \geq a) \leq \frac{\operatorname{Var}(X)}{a^2} \]
  \end{proposition}

  \reference{195}
  \dev{theoreme-de-weierstrass-par-les-probabilites}

  \begin{theorem}[Bernstein]
    Soit $f : [0,1] \rightarrow \mathbb{R}$ continue. On note
    \[ \forall n \in \mathbb{N}^*, \, B_n(f) : x \mapsto \sum_{k=0}^n \binom{n}{k} f \left( \frac{k}{n} \right) x^k (1-x)^{n-k} \]
    le $n$-ième polynôme de Bernstein associé à $f$. Alors le suite de fonctions $(B_n(f))$ converge uniformément vers $f$.
  \end{theorem}

  \begin{theorem}[Weierstrass]
    Toute fonction continue $f : [a,b] \rightarrow \mathbb{R}$ (avec $a, b \in \mathbb{R}$ tels que $a \leq b$) est limite uniforme de fonctions polynômiales sur $[a, b]$.
  \end{theorem}

  \subsection{Théorèmes limites et d'approximations}

  \subsubsection{Théorèmes limites}

  \reference[Z-Q]{544}

  \begin{theorem}[Lévy]
    Soient $(X_n)$ une suite de variables aléatoires réelles et $X$ une variable aléatoire réelle. Alors :
    \[ X_n \overset{(d)}{\longrightarrow} X \iff \phi_{X_n} \text{ converge simplement vers } \phi_X \]
    où $\phi_Y$ désigne la fonction caractéristique d'une variable aléatoire réelle $Y$.
  \end{theorem}

  \reference[G-K]{307}

  \begin{theorem}[Central limite]
    Soit $(X_n)$ une suite de variables aléatoires réelles indépendantes de même loi admettant un moment d'ordre $2$. On note $m$ l'espérance et $\sigma^2$ la variance commune à ces variables. On pose $S_n = X_1 + \dots + X_n - nm$. Alors,
    \[ \left ( \frac{S_n}{\sqrt{n}} \right) \overset{(d)}{\longrightarrow} \mathcal{N}(0, \sigma^2) \]
  \end{theorem}

  \reference{390}
  \dev{theoreme-des-evenements-rares-de-poisson}

  \begin{application}[Théorème des événements rares de Poisson]
    Soit $(N_n)_{n \geq 1}$ une suite d'entiers tendant vers l'infini. On suppose que pour tout $n$, $A_{n,N_1}, \dots , A_{n,N_n}$ sont des événements indépendants avec $\mathbb{P}(A_{n,N_k}) = p_{n,k}$. On suppose également que :
    \begin{enumerate}[label=(\roman*)]
      \item $\lim_{n \rightarrow +\infty} s_n = \lambda > 0$ où $\forall n \in \mathbb{N}, s_n = \sum_{k=1}^{N_n} p_{n,k}$.
      \item $\lim_{n \rightarrow +\infty} \sup_{k \in \llbracket 1, N_n \rrbracket} p_{n,k} = 0$.
    \end{enumerate}
    Alors, la suite de variables aléatoires $(S_n)$ définie par
    \[ \forall n \in \mathbb{N}^*, \, S_n = \sum_{k=1}^n \mathbb{1}_{A_{n,k}} \]
    converge en loi vers la loi de Poisson de paramètre $\lambda$.
  \end{application}

  \reference{270}

  \begin{theorem}[Loi faible des grands nombres]
    Soit $(X_n)$ une suite de variables aléatoires deux à deux indépendantes de même loi et $\mathcal{L}_1$. On pose $M_n = \frac{X_1 + \dots + X_n}{n}$. Alors,
    \[ M_n \overset{(p)}{\longrightarrow} \mathbb{E}(X_1) \]
  \end{theorem}

  \reference[Z-Q]{532}

  \begin{theorem}[Loi forte des grands nombres]
    Soit $(X_n)$ une suite de variables aléatoires mutuellement indépendantes de même loi. On pose $M_n = \frac{X_1 + \dots + X_n}{n}$. Alors,
    \[ X_1 \in \mathcal{L}_1 \iff M_n \overset{(ps.)}{\longrightarrow} \ell \in \mathbb{R} \]
    Dans ce cas, on a $\ell = \mathbb{E}(X_1)$.
  \end{theorem}

  \subsubsection{Approximation d'une loi normale}

  \reference[G-K]{308}

  \begin{theorem}[Moivre-Laplace]
    Soit $(X_n)$ une suite de variables aléatoires indépendantes de même loi $\mathcal{B}(p)$. Alors,
    \[ \frac{\sum_{k=1}^{n} X_k - np}{\sqrt{n}} \overset{(d)}{\longrightarrow} \mathcal{N}(0, p(1-p)) \]
  \end{theorem}

  \subsubsection{Approximation d'une loi de Poisson}

  \reference{297}

  \begin{theorem}
    Soit, pour $n \geq 1$, une variable aléatoire $X_n$ suivant la loi binomiale de paramètres $n$ et $p_n$. On suppose que $\lim_{n \rightarrow +\infty} n p_n = \lambda > 0$.
    Alors,
    \[ X_n \overset{(d)}{\longrightarrow} \mathcal{P}(\lambda) \]
  \end{theorem}

  \begin{remark}
    En pratique, pour $n \geq 30$ et $np \leq 10$, on a une ``bonne'' approximation de $\mathcal{P}(\lambda)$.
  \end{remark}

  \reference[GOU21]{343}

  \begin{example}
    Si chaque seconde, il y a une probabilité $p = \frac{1}{600}$ qu'un client entre dans un magasin, le nombre de clients qui entrent sut un intervalle d'une heure suit approximativement une loi de Poisson de paramètre $\lambda = 3600p = 6$.
    \newpar
    Pour cette raison, on appelle parfois cette loi la \textit{loi des événements rares}.
  \end{example}

  \reference[G-K]{297}

  \begin{application}[Nombre de dérangements]
    Soit $\sigma_n$ une permutation aléatoire suivant la loi uniforme sur $S_n$. Si on note $D_n$ le nombre de points fixes de $\sigma_n$, on a
    \[ \mathbb{P}(D_n = k) = \frac{1}{k!} \frac{d_{n-k}}{(n-k)!} \]
    où $d_n$ est le nombre de permutations de $S_n$ sans point fixe. En particulier, comme $d_n \sim \frac{1}{e} n!$, on a
    \[ D_n \overset{(d)}{\longrightarrow} \mathcal{P}(1) \]
  \end{application}
  %</content>
\end{document}

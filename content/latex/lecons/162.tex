\input{../common}
\input{../bibliography}

\begin{document}
  %<*content>
  \lesson{algebra}{162}{Systèmes d'équations linéaires ; opérations élémentaires, aspects algorithmiques et conséquences théoriques.}

  Soit $\mathbb{K}$ un corps commutatif.

  \subsection{Généralités}

  \subsubsection{Définitions}

  \reference[GRI]{143}

  \begin{definition}
    \label{162-1}
    \begin{itemize}
      \item On appelle \textbf{système linéaire} à $p$ équations en $n$ inconnues, un système d'équations de la forme
      \[
      \begin{cases}
        a_{1,1} x_1 + \dots + a_{1,n} x_n = b_1 \\
        \vdots \\
        a_{p,1} x_1 + \dots + a_{1,n} x_n = b_p \\
      \end{cases}
      \tag{$S$}
      \]
      où $\forall i \in \llbracket 1, p \rrbracket, \, \forall j \in \llbracket 1, n \rrbracket, \, a_{i,j} \in \mathbb{K}, \, b_i \in \mathbb{K}$.
      \item On appelle \textbf{solution} de $(S)$ tout vecteur $x = (x_1, \dots, x_n) \in \mathbb{K}^n$ dont les composantes $(x_i)_{i \in \llbracket 1, n \rrbracket}$ satisfont toutes les équations.
      \item $(S)$ est dit \textbf{compatible} s'il admet au moins une solution.
    \end{itemize}
  \end{definition}

  \reference{38}

  \begin{example}
    Le système ci-dessous est linéaire,
    \[
      \begin{cases}
        2x + y - 2z + 3w = 1 \\
        3x + 2y - z + 2w = 4 \\
        3x + 3y + 3z - 3w = 5
      \end{cases}
    \]
    il n'est pas compatible.
  \end{example}

  \subsubsection{Écriture sous forme matricielle}

  \reference{144}

  \begin{proposition}
    On considère le système $(S)$ de la \cref{162-1}. On peut l'écrire sous forme matricielle
    \[ AX = B \tag{$S$} \]
    avec $A = (a_{i,j})_{\substack{i \in \llbracket 1, p \rrbracket \\ j \in \llbracket 1, n \rrbracket}} \in \mathcal{M}_{p,n}(\mathbb{K})$, $X = (x_j)_{j \in \llbracket 1, n \rrbracket} \in \mathcal{M}_{n,1}(\mathbb{K})$ et $B = (b_i)_{i \in \llbracket 1, p \rrbracket} \in \mathcal{M}_{p,1}(\mathbb{K})$.
  \end{proposition}

  \begin{definition}
    On appelle \textbf{rang} du système $(S)$ le rang de la matrice $A$.
  \end{definition}

  \begin{proposition}
    Soit
    \[ AX = B \]
    un système linéaire à $n$ équations et $n$ inconnues. Si $A$ est inversible, on a une unique solution, donnée par $X = A^{-1}B$.
  \end{proposition}

  \subsection{Étude de systèmes particuliers}

  \subsubsection{Systèmes de Cramer}

  \begin{definition}
    On appelle \textbf{système de Cramer}, un système linéaire à $n$ équations et $n$ inconnues $AX = B$.
  \end{definition}

  \begin{theorem}[Formules de Cramer]
    Un système de Cramer $AX = B$ admet une unique solution donnée par $X = (x_i)_{i \in \llbracket 1, n \rrbracket}$ où
    \[ \forall i \in \llbracket 1, n \rrbracket, \, x_i = \frac{\det(A_i)}{\det(A)} \]
    avec $A_i$ obtenue en remplaçant la $i$-ième colonne de $A$ par $B$.
  \end{theorem}

  \begin{example}
    Le système
    \[
      \begin{cases}
        2x - 5y + 2z = 7 \\
        x + 2y - 4z = 3 \\
        3x - 4y - 6z = 5
      \end{cases}
    \]
    est de Cramer, son unique solution est $(5,1,1)$.
  \end{example}

  \subsubsection{Équations de Sylvester}

  \reference[I-P]{177}

  \begin{lemma}
    Soit $\Vert . \Vert$ une norme d'algèbre sur $\mathcal{M}_n(\mathbb{C})$, et soit $A \in \mathcal{M}_n(\mathbb{C})$ une matrice dont les valeurs propres sont de partie réelle strictement négative. Alors il existe une fonction polynômiale $P : \mathbb{R} \rightarrow \mathbb{R}$ et $\lambda > 0$ tels que $\Vert e^{tA} \Vert \leq e^{- \lambda t} P(t)$.
  \end{lemma}

  \dev{equation-de-sylvester}

  \begin{application}[Équation de Sylvester]
    Soient $A$ et $B \in \mathcal{M}_n(\mathbb{C})$ deux matrices dont les valeurs propres sont de partie réelle strictement négative. Alors pour tout $C \in \mathcal{M}_n(\mathbb{C})$, l'équation $AX + XB = C$ admet une unique solution $X$ dans $\mathcal{M}_n(\mathbb{C})$.
  \end{application}

  \subsection{Méthodes générales de résolution}

  \subsubsection{Théorème de Rouché-Fontené}

  \reference[GOU21]{144}

  \begin{lemma}
    Soit $A \in \mathcal{M}_{p,n}(\mathbb{K})$. Soit $r = \rang(A)$. Il existe un déterminant $\Delta$ d'ordre $r$ extrait de $A$.
  \end{lemma}

  \begin{definition}
    \begin{itemize}
      \item Le déterminant $\Delta$ précédent est le \textbf{déterminant principal} de $A$.
      \item Les équations (resp. inconnues) dont les indices sont deux des lignes (resp. colonnes) de $\Delta$ s'appellent les \textbf{équations principales} (resp. \textbf{inconnues principales}).
      \item Si $\Delta = \det(a_{i,j})_{\substack{i \in I \\ j \in J}}$, on appelle \textbf{déterminants caractéristiques} les déterminants d'ordre $r+1$ de la forme
      \begin{center}
        \begin{tabular}{|c|c|}
          $(a_{i,j})_{\substack{i \in I \\ j \in J}}$ & $(b_i)_{i \in I}$ \\
          \hline
          $(a_{k,j})_{j \in J}$ & $b_k$
        \end{tabular}
        avec $k \notin J$.
      \end{center}
    \end{itemize}
  \end{definition}

  \begin{theorem}[Rouché-Fontené]
    Un système de rang $r$
    \[ AX = B \]
    avec $A \in \mathcal{M}_{p,n}(\mathbb{K})$, $X \in \mathcal{M}_{n,1}(\mathbb{K})$ et $B \in \mathcal{M}_{1,p}(\mathbb{K})$ admet des solutions si et seulement si $p = r$ ou les $p-r$ déterminants caractéristiques sont nuls. Le système est alors équivalent au système des équations principales. Les inconnues principales étant déterminées par un système de Cramer à l'aide des inconnues non principales.
  \end{theorem}

  \begin{example}
    Si,
    \[
    (S) \iff
    \begin{cases}
      x + 2y + z + t = 1 \\
      x - z - t = 1 \\
      -x + y + z + 2t = m
    \end{cases}
    \quad
    m \in \mathbb{R}
    \]
    on a $\rang(A) = 2$, $(S)$ admet des solutions si et seulement si $m=-1$, et
    \[
    (S)
    \iff \begin{cases}
      x + 2y = 1 + z - t \\
      x = 1 + z + t
    \end{cases}
    \iff \begin{cases}
      x = 1 + z + t \\
      y = -t
    \end{cases}
    \]
  \end{example}

  \subsubsection{Algorithme du pivot de Gauss}

  \paragraph{Opérations élémentaires}

  \reference[ROM21]{186}

  \begin{definition}
    Soit $A = (a_{i,j})_{\substack{i \in \llbracket 1, n \rrbracket \\ j \in \llbracket 1, m \rrbracket}} \in \mathcal{M}_{n,m}(\mathbb{K})$.
    \begin{itemize}
      \item On dit que $A$ est \textbf{échelonnée en lignes} si elle est nulle ou si elle est non nulle et il existe un entier $r$ compris entre $1$ et $n$ tel que :
      \begin{itemize}
        \item Les $r$ premières lignes de $A$ sont non nulles.
        \item Les $n-r$ dernières lignes de $A$ sont nulles.
        \item En notant $\forall i \in \llbracket 1, n \rrbracket, \, d_i = \min\{ j \in \llbracket 1, m \rrbracket \mid a_{i,j} \neq 0 \}$, on a
        \[ 1 \leq d_1 < d_2 < \dots < d_r \leq m \]
      \end{itemize}
      \item On dit que $A$ est \textbf{échelonnée en colonnes} si $\tr{A}$ est échelonnée en lignes.
    \end{itemize}
  \end{definition}

  \begin{proposition}
    Avec les notations précédentes, si $A$ est échelonnée en lignes, alors $\rang(A) = r$.
  \end{proposition}

  \begin{definition}
    On dit qu'un système linéaire $AX = B$ est \textbf{échelonné} si la matrice $A$ est échelonnée en lignes.
  \end{definition}

  \begin{theorem}
    \begin{enumerate}[label=(\roman*)]
      \item La multiplication à gauche par une matrice de dilatation $D_i(\lambda)$ (obtenue à partir de la matrice identité en remplaçant le coefficient $1$ à la $i$-ième ligne par $\lambda$) a pour effet de multiplier la ligne $i$ par $\lambda$.
      \item La multiplication à gauche par une matrice de transvection $T_{i,j}(\lambda)$ (obtenue à partir de la matrice identité en remplaçant le coefficient $0$ à la $i$-ième ligne et $j$-ième colonne par $\lambda$) a pour effet de multiplier la ligne $i$ par la somme de la ligne $i$ et de la ligne $j$ multipliée par $\lambda$.
      \item La multiplication à droite fait des effets similaires sur les colonnes.
    \end{enumerate}
  \end{theorem}

  \begin{remark}
    Pour effectuer une permutation des lignes $i$ et $j$, il suffit de multiplier à gauche par
    \[ D_j(-1)T_{i,j}(1)T_{j,i}(-1)T_{i,j}(1) \]
  \end{remark}

  \begin{definition}
    On appelle \textbf{opération élémentaire} une des opérations citées précédemment.
  \end{definition}

  \begin{theorem}
    Une opération élémentaire sur les lignes d'un système linéaire le transforme en un système équivalent.
  \end{theorem}

  \paragraph{Résolution pratique}

  On cherche à résoudre  le système linéaire de $n$ équations à $m$ inconnues :

  \[
  \begin{cases}
    a_{1,1} x_1 + \dots + a_{1,m} x_m = b_1 \\
    \vdots \\
    a_{n,1} x_1 + \dots + a_{1,n} x_m = b_n \\
  \end{cases}
  \tag{$S$}
  \]

  mis sous forme matricielle $AX = B$. On suppose $A \neq 0$ (sinon l'ensemble des solutions de $(S)$ est $\mathbb{K}^n$). On note $L_i$ la ligne numéro $i$ de $A$ pour tout $i \in \llbracket 1, m \rrbracket$.

  \begin{theorem}[Échelonnement par pivot]
    \begin{enumerate}[label=(\roman*)]
      \item Si les premières colonnes $C_1, \dots, C_{d_1}$ de la matrice $A$ sont nulles, les variables $x_1, \dots, x_d$ peuvent être quelconques et on passe à la colonne $d_1 + 1$, ce qui nous donne un système de $n$ équations à $m-d$ inconnues. On suppose donc que la première colonne de la matrice $A$ n'est pas nulle et en permutant la ligne $1$ avec une des lignes suivantes on se ramène à un système $A^{(1)}X = B^{(1)}$ avec $a_{1,1}^{(1)} \neq 0$. On élimine alors $x_1$ des lignes $2$ à $n$ en effectuant les opérations élémentaires $L_i \leftarrow L_i - \frac{a_{1,1}^{(1)}}{a_{1,1}^{(1)}} L_i$ pour $i \in \llbracket 2, n \rrbracket$, ce qui donne le système :
      \[
      \begin{cases}
        a_{1,1}^{(1)} x_1 + a_{1,2}^{(1)} x_2 + \dots + a_{1,m}^{(1)} x_m = b_1^{(1)} \\
        a_{2,2}^{(2)} x_2 + \dots + a_{2,m}^{(2)} x_m = b_2^{(2)} \\
        \vdots \\
        a_{n,2}^{(2)} x_1 + \dots + a_{1,n}^{(2)} x_m = b_n^{(2)} \\
      \end{cases}
      \]
      \item \begin{itemize}
        \item Si l'un  des coefficients $a_{i,2}^{(2)}$ est non nul (pour $i \in \llbracket 1, n \rrbracket$), on recommence avec un procédé analogue pour éliminer $x_2$ des équations $3$ à $n$.
        \item Si tous les coefficients $a_{i,2}^{(2)}$ sont nuls, on passe alors à la colonne suivante. Et on recommence ainsi de suite.
      \end{itemize}
      \item Au bout d'un nombre fini d'étapes, on aboutit à un système échelonné qui est équivalent au système $(S)$.
    \end{enumerate}
  \end{theorem}

  Une fois le système échelonné, on peut procéder à la résolution.

  \begin{theorem}[Remontée]
    Trois cas de figure sont possibles.
    \begin{enumerate}[label=(\roman*)]
      \item Soit on a obtenu un système de Cramer triangulaire supérieur d'ordre $n = m = r$. Un tel système a une unique solution et se résout alors ``en remontée''. Dans ce cas, $A$ est de rang $n = m$.
      \item Soit on a obtenu un système de $r \leq n$ équations à $m > r$ inconnues. Les $r$ premières inconnues sont les inconnues principales et les autres inconnues non principales. Ces dernières sont alors utilisées comme paramètres en second membre et on résout le système d'inconnues principales $x_1, \dots, x_r$. L'ensemble des solutions est un espace affine de dimension $n-r$ et le rang de la matrice est $r$.
      \item Soit on a obtenu un système de $n$ équations à $r = m < n$ inconnues, de la forme :
      \[
      \begin{cases}
        \alpha_{1,1} x_1 + \alpha_{1,2} x_2 + \dots + \alpha_{1,m} x_m = \beta_1 \\
        \alpha_{2,2} x_2 + \dots + \alpha_{2,m} x_m = \beta_2 \\
        \vdots \\
        \alpha_{n,2} x_1 + \dots + \alpha_{1,n} x_m = \beta_m \\
        0 = \beta_{m+1} \\
        \vdots \\
        0 = \beta_n
      \end{cases}
      \]
      si l'un des $\beta_i$ pour $i \in \llbracket m+1, n \rrbracket$ est non nul, alors le système est incompatible. Sinon, le système des équations principales est un système de Cramer, qui admet une unique solution. Dans ce cas, la matrice est de rang $m$.
    \end{enumerate}
  \end{theorem}

  \reference[GRI]{38}

  \begin{example}
    \[
      \begin{cases}
        x + 2y - z = 1 \\
        2x + 3y + z = 2 \\
        x + 4y - 6z = 2
      \end{cases}
      \iff
      \begin{cases}
        x + 2y - z = 1 \\
        -y + 3z = 0 \\
        2y - 5z = 1
      \end{cases}
      \iff
      \begin{cases}
        x + 2y - z = 1 \\
        -y + 3z = 0 \\
        z = 1
      \end{cases}
    \]
    On a pour unique solution $(-4,3,1)$.
  \end{example}

  \paragraph{Comparaison avec les formules de Cramer}

  \reference[FFN]{38}

  \begin{theorem}
    L'algorithme du pivot de Gauss pour résoudre un système $AX = B$ a un complexité de $O(n^3)$. L'utilisation des formules de Cramer se fait en $O(n(n+1)!)$.
  \end{theorem}

  \subsubsection{Conséquences théoriques}

  \paragraph{Familles libres}

  \reference[GRI]{44}

  \begin{proposition}
    La méthode du pivot permet de décider si une famille est libre (en échelonnant la matrice formée des vecteurs de cette famille).
  \end{proposition}

  \begin{example}
    La famille
    \[ \begin{pmatrix} 1 \\ -2 \\ -3 \end{pmatrix}, \, \begin{pmatrix} 2 \\ 3 \\ -1 \end{pmatrix}, \, \begin{pmatrix} 3 \\ 2 \\ 1 \end{pmatrix} \]
    est libre.
  \end{example}

  \paragraph{Rang, équivalence}

  \reference[C-G]{24}

  \begin{proposition}
    Soit $M \in \mathcal{M}_{n,m}(\mathbb{K})$ de rang $r$. Par des opérations élémentaires sur des lignes et des colonnes de $M$, on peut la transformer en
    \[
      \begin{pmatrix}
        I_r & 0 \\
        0 & 0
      \end{pmatrix}
    \]
  \end{proposition}

  \paragraph{Commutant d'une matrice}

  Soit $A \in \mathcal{M}_n(\mathbb{K})$.

  \reference[GOU21]{289}

  \begin{lemma}
    Si $\pi_A = \chi_A$, alors $A$ est cyclique :
    \[ \exists x \in \mathbb{K}^n \setminus \{ 0 \} \text{ tel que } (x, Ax, \dots, A^{n-1}x) \text{ est une base de } \mathbb{K}^n \]
  \end{lemma}

  \reference[FGN2]{160}

  \begin{notation}
    \begin{itemize}
      \item On note $\mathcal{T}_n(\mathbb{K})$ l'ensemble des matrices carrées triangulaires supérieures d'ordre $n$ à coefficients dans le corps $\mathbb{K}$.
      \item On note $\mathcal{C}(A)$ le commutant de $A$.
    \end{itemize}
  \end{notation}

  \begin{lemma}
    \[ \dim_{\mathbb{K}}(\mathcal{C}(A)) \geq n \]
  \end{lemma}

  \begin{lemma}
    Le rang de $A$ est invariant par extension de corps.
  \end{lemma}

  \dev{dimension-du-commutant}

  \begin{theorem}
    \[ \mathbb{K}[A] = \mathcal{C}(A) \iff \pi_A = \chi_A \]
  \end{theorem}

  \subsection{Décompositions}

  \subsubsection{Décomposition LU}

  \reference[ROM21]{690}

  \begin{definition}
    Les \textbf{sous-matrices principales} d'une matrice $(a_{i,j})_{i,j \in \llbracket 1, n \rrbracket} \in \mathcal{M}_n(\mathbb{K})$ sont les matrices $A_k = (a_{i,j})_{i,j \in \llbracket 1, k \rrbracket} \in \mathcal{M}_k(\mathbb{K})$ où $k \in \llbracket 1, n \rrbracket$. Les \textbf{déterminants principaux} sont les déterminants des matrices $A_k$, pour $k \in \llbracket 1, n \rrbracket$.
  \end{definition}

  \begin{theorem}[Décomposition lower-upper]
    \label{162-2}
    Soit $A \in \mathrm{GL}_n(\mathbb{K})$. Alors, $A$ admet une décomposition
    \[ A = LU \]
    (où $L$ est une matrice triangulaire inférieure à diagonale unité et $U$ une matrice triangulaire supérieure) si et seulement si tous les déterminants principaux de $A$ sont non nuls. Dans ce cas, une telle décomposition est unique.
  \end{theorem}

  \begin{corollary}
    Soit $A \in \mathrm{GL}_n(\mathbb{K}) \, \cap \, \mathcal{S}_n(\mathbb{K})$. Alors, on a l'unique décomposition de $A$ :
    \[ A = LD\tr{L} \]
    où $L$ est une matrice triangulaire inférieure et $D$ une matrice diagonale.
  \end{corollary}

  \begin{application}[Décomposition de Cholesky]
    Soit $A \in \mathcal{M}_n(\mathbb{R})$. Alors, $A \in \mathcal{S}_n^{++}(\mathbb{R})$ si et seulement s'il existe $B \in \mathrm{GL}_n(\mathbb{R})$ triangulaire inférieure telle que $A = B\tr{B}$. De plus, une telle décomposition est unique si on impose la positivité des coefficients diagonaux de $B$.
  \end{application}

  \reference[GRI]{368}

  \begin{example}
    On a la décomposition de Cholesky :
    \[ \begin{pmatrix} 1 & 2 \\ 2 & 5 \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 2 & 1 \end{pmatrix} \begin{pmatrix} 1 & 2 \\ 0 & 1 \end{pmatrix} \]
  \end{example}

  \reference[C-G]{257}

  \begin{proposition}
    Soit $A \in \mathrm{GL}_n(\mathbb{K})$ vérifiant les hypothèses du \cref{162-2}. On définit la suite $(A_k)$ où $A_0 = A$ et $\forall k \in \mathbb{N}$, $A_{k+1}$ est la matrice obtenue à partir de $A_k$ à l'aide du pivot de Gauss sur la $(k+1)$-ième colonne. Alors, $A_{n-1}$ est la matrice $U$ de la décomposition $A = LU$ du \cref{162-2}.
  \end{proposition}

  \begin{remark}
    Pour résoudre un système linéaire $AX = Y$, on se ramène à $A = LU$ en $O \left( \frac{2}{3}n^3 \right)$. Puis, on résout deux systèmes triangulaires ``en cascade'' :
    \[ LX' = Y \text{ puis } UX = X' \]
    ceux-ci demandant chacun $O(2n^2)$ opérations.
  \end{remark}

  \begin{theorem}[Décomposition PLU]
    Soit $A \in \mathrm{GL}_n(\mathbb{K})$. Alors, il existe $P \in \mathrm{GL}_n(\mathbb{K})$, matrice de permutations, telle que $P^{-1}A$ admet une décomposition $LU$.
  \end{theorem}

  \subsubsection{Décomposition QR}

  \reference[ROM21]{692}

  \begin{theorem}[Décomposition QR]
    Soit $A \in \mathrm{GL}_n(\mathbb{R})$. Alors, $A$ admet une décomposition
    \[ A = QR \]
    où $Q$ est une matrice orthogonale et $R$ est une matrice triangulaire supérieure à coefficients diagonaux strictement positifs. On a unicité d'une telle décomposition.
  \end{theorem}

  \begin{corollary}[Théorème d'Iwasawa]
    Soit $A \in \mathrm{GL}_n(\mathbb{R})$. Alors, $A$ admet une décomposition
    \[ A = QDR \]
    où $Q$ est une matrice orthogonale, $D$ est une matrice diagonale à coefficients strictement positifs et $R$ est une matrice triangulaire supérieure à coefficients diagonaux égaux à $1$. On a unicité d'une telle décomposition.
  \end{corollary}

  \reference[GRI]{272}

  \begin{example}
    On a la factorisation QR suivante,
    \[ \begin{pmatrix} 0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0 \end{pmatrix} = \left( \frac{1}{\sqrt{6}} \begin{pmatrix} 0 & 2 & \sqrt{2} \\ \sqrt{3} & -1 & \sqrt{2} \\ \sqrt{3} & 1 & -\sqrt{2} \end{pmatrix} \right) \left( \frac{1}{\sqrt{6}} \begin{pmatrix} 2\sqrt{3} & \sqrt{3} & 1 \\ 0 & 3 & 1 \\ 0 & 0 & 2\sqrt{2} \end{pmatrix} \right) \]
    qui peut être obtenue via un procédé de Gram-Schmidt.
  \end{example}

  \reference{368}

  \begin{remark}
    Pour résoudre un système linéaire $AX = Y$, si l'on a trouvé une telle factorisation $A = QR$, on résout
    \[ RX = \tr{Q}Y \]
    c'est-à-dire, un seul système triangulaire (contre deux pour la factorisation LU).
  \end{remark}
  %</content>
\end{document}
